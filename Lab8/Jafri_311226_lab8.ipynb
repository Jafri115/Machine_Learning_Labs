{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Course Machine Learning\n",
    "# Exercise Sheet 8\n",
    "January 5th, 2022\n",
    "\n",
    "Syed Wasif Murtaza Jafri-311226\n",
    "\n",
    "## Exercise 1: Optical Character Recognition via Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "plt.rcParams['figure.figsize'] = (10 ,8)\n",
    "from sympy import symbols, diff\n",
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import math\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = load_digits(as_frame=False)\n",
    "#plt.gray()\n",
    "plt.matshow(digits.images[110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = digits.target\n",
    "Y = Y.reshape(len(Y),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y,test_size=0.2,random_state=3116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=3116, max_iter=300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.54100615e-08, 2.62938165e-08, 2.13561410e-15, 4.73356379e-20,\n",
       "        9.99999868e-01, 3.41843016e-11, 1.40238884e-08, 6.61206535e-08,\n",
       "        6.16084626e-12, 1.16212870e-11]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 2, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(train_x, train_y,hidden_layer_sizes_range,batch_size_range):\n",
    "    param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_range, batch_size=batch_size_range)\n",
    "\n",
    "    cv = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42) # stratifying sample for crossvalidation as test size 20% and divinding it into two parts\n",
    "    # for param grid calling GridSearchCV with penalty(for specifying model) and diff hyper params in grid\n",
    "    grid = GridSearchCV(MLPClassifier(random_state=3116, max_iter=500), param_grid=param_grid, cv=cv,return_train_score=True, n_jobs=14, verbose=10) \n",
    "    grid.fit(train_x, train_y)\n",
    "\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "    scores = []\n",
    "    # to calculate test and train score for each setting in parameter and adding it to scores list\n",
    "    for i,j,k in zip (grid.cv_results_['params'],grid.cv_results_['mean_test_score'],grid.cv_results_['mean_train_score']):\n",
    "        scores.append([j,k,i['hidden_layer_sizes'],i['batch_size']])\n",
    "    hidden_layer_sizes_best = grid.best_params_['hidden_layer_sizes']\n",
    "    batch_sizebest = grid.best_params_['batch_size']\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes =hidden_layer_sizes_best,batch_size=batch_sizebest)\n",
    "\n",
    "    return clf,np.array(scores),hidden_layer_sizes_best,batch_sizebest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=14)]: Done   4 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=14)]: Done  13 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=14)]: Done  22 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=14)]: Done  33 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=14)]: Done  40 out of  60 | elapsed:   17.9s remaining:    8.9s\n",
      "[Parallel(n_jobs=14)]: Done  47 out of  60 | elapsed:   19.7s remaining:    5.4s\n",
      "[Parallel(n_jobs=14)]: Done  54 out of  60 | elapsed:   21.0s remaining:    2.2s\n",
      "[Parallel(n_jobs=14)]: Done  60 out of  60 | elapsed:   21.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'batch_size': 16, 'hidden_layer_sizes': 100} with a score of 0.98\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes_range = [2, 16, 32, 64,100]\n",
    "batch_size_range =[16, 32, 64,128,264,528]\n",
    "clf,scores,hidden_layer_sizes_best,batch_sizebest = get_best_params(X_train, y_train,hidden_layer_sizes_range,batch_size_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=16, hidden_layer_sizes=100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy::',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: batch_size = 16 and  hidden_layer_sizes = 100\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are: batch_size = %i and  hidden_layer_sizes = %i\" % (batch_sizebest,hidden_layer_sizes_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: End-to-End Self-Driving via Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #defaultdirectorywheredataisloaded\n",
    "        self.filepath='driving_dataset/'\n",
    "        self.filenames=os.listdir(self.filepath)\n",
    "        self.xs = []\n",
    "        self.ys = []\n",
    "        with open(\"angles.txt\") as f:\n",
    "            for line in f:\n",
    "                self.xs.append(\"driving_dataset/\" + line.split()[0])\n",
    "                self.ys.append(float(line.split()[1]) * 3.14159265 / 180)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        filename=self.filenames[index]\n",
    "        img=cv2.imread(self.filepath+filename)\n",
    "        height, width, channels = img.shape\n",
    "        #print(height,width,channels)\n",
    "        #Resizingimagesto(32,32)\n",
    "        resized=cv2.resize(img,(66,200),interpolation=cv2.INTER_AREA)\n",
    "        transform_norm = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        #to Tensor\n",
    "        tensorImg2= transform_norm(resized)\n",
    "     \n",
    "        mean = tensorImg2.mean([1,2])\n",
    "        std = tensorImg2.std([1,2])\n",
    "\n",
    "        transform_norm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "        norm_tensorImg2= transform_norm(resized)\n",
    "\n",
    "        return norm_tensorImg2.float(),torch.tensor([self.ys[index]])\n",
    "        \n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 24, 5,stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(24, 36, 5,stride=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(36, 48, 5,stride=(2, 2))\n",
    "        self.conv4 = nn.Conv2d(48, 64, 3)\n",
    "        self.conv5 = nn.Conv2d(64, 64, 3)\n",
    "        self.fc1 = nn.Linear(64 * 18 * 1, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.out = nn.Linear(10, 1)\n",
    "    def forward(self,x):\n",
    "        # \n",
    "        # input x size : torch.Size([60, 3, 200, 66])\n",
    "        \n",
    "        # 5 Convolution Layers\n",
    "        x = F.relu(self.conv1(x)) # 1 CV 1 torch.Size([60, 24, 98, 31])\n",
    "        x = F.relu(self.conv2(x)) #  CV 2  torch.Size([60, 36, 47, 14])\n",
    "        x = F.relu(self.conv3(x)) #  CV 3  torch.Size([60, 36, 47, 14])\n",
    "        x = F.relu(self.conv4(x)) # #  CV 4  torch.Size([60, 64, 20, 3])\n",
    "        x = F.relu(self.conv5(x)) #  CV 5  torch.Size([60, 64, 18, 1])\n",
    "        \n",
    "        x = x.view(-1, 64 * 18 * 1) # flatten  torch.Size([60, 1152])\n",
    "        \n",
    "        # 3 Fully connected  Layers\n",
    "        x = F.relu(self.fc1(x)) # torch.Size([60, 100])\n",
    "        x = F.relu(self.fc2(x)) #  torch.Size([60, 50])\n",
    "        x = F.relu(self.fc3(x)) #  torch.Size([60, 10])\n",
    "        \n",
    "        # Output layer with activation function\n",
    "        x = F.tanh(self.out(x)) #  out  torch.Size([60, 1])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset=Dataset()\n",
    "\n",
    "batch_size = 60\n",
    "shuffle_dataset = False\n",
    "random_seed= 31142\n",
    "num_epochs = 5\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "testSplitRatio = 10000/dataset_size\n",
    "validationSplitRatio = 10000/dataset_size\n",
    "trainSplitRation = 1-testSplitRatio-validationSplitRatio\n",
    "\n",
    "train_indices = indices[0:math.floor(dataset_size*trainSplitRation)]\n",
    "val_indices = indices[math.floor(dataset_size*trainSplitRation):math.floor(dataset_size*(trainSplitRation+validationSplitRatio))]\n",
    "test_indices = indices[math.floor(dataset_size*(trainSplitRation+validationSplitRatio)):]\n",
    "\n",
    "\n",
    "train_sampler = SequentialSampler(train_indices)\n",
    "valid_sampler = SequentialSampler(val_indices)\n",
    "test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler,shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                sampler=test_sampler,shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [200/427], Loss: 0.0612\n",
      "Epoch [1/5], Step [400/427], Loss: 0.0335\n",
      "Epoch [2/5], Step [200/427], Loss: 0.0679\n",
      "Epoch [2/5], Step [400/427], Loss: 0.0315\n",
      "Epoch [3/5], Step [200/427], Loss: 0.0732\n",
      "Epoch [3/5], Step [400/427], Loss: 0.0284\n",
      "Epoch [4/5], Step [200/427], Loss: 0.0737\n",
      "Epoch [4/5], Step [400/427], Loss: 0.0277\n",
      "Epoch [5/5], Step [200/427], Loss: 0.0742\n",
      "Epoch [5/5], Step [400/427], Loss: 0.0271\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "net=ConvNet().to(device)\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=1e-3)\n",
    "criterion=torch.nn.MSELoss()\n",
    "for epoch in range(num_epochs):\n",
    "    for i,sample_batched in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        yhat=net(sample_batched[0].cuda())\n",
    "        loss=criterion(yhat.squeeze(),sample_batched[1].squeeze().cuda())\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if (i+1) % 200 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0077, device='cuda:0')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MSELoss(yhat,y):\n",
    "    return torch.mean((yhat-y)**2)\n",
    "criterion=MSELoss\n",
    "mse = 0\n",
    "with torch.no_grad():\n",
    "    for i,sample_batched in enumerate(test_loader):\n",
    "        optimizer.zero_grad()\n",
    "        yhat=net(sample_batched[0].cuda())\n",
    "        loss=criterion(yhat.squeeze(),sample_batched[1].squeeze().cuda())\n",
    "        mse += loss\n",
    "    rmse = torch.sqrt (mse)\n",
    "    rmseAvg = rmse / len(test_loader)\n",
    "        \n",
    "\n",
    "rmseAvg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00759779242798686"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmseAvg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test images 0.00759779242798686\n"
     ]
    }
   ],
   "source": [
    "print ('RMSE on test images', rmseAvg.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
